{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f17f97b6",
   "metadata": {},
   "source": [
    "## Dummy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6430f602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jayhong7200/2025_MIMIC_IV/mimic_venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2687de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy data loaded from Results/Dummy/input_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "dummy_input_dict_path = Path('Results/Dummy/input_dict.pkl')\n",
    "dummy_input_dict_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not dummy_input_dict_path.exists() :\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    text_tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "    # Dummy\n",
    "    age_dummy = torch.tensor([0])\n",
    "    gender_dummy = torch.tensor([0])\n",
    "    ethnicity_dummy = torch.tensor([0])\n",
    "    types_dummy = torch.tensor([0]*23).unsqueeze(0)\n",
    "    codes_dummy = torch.tensor([0]*23).unsqueeze(0)\n",
    "    tabular_flag = torch.tensor([1])\n",
    "\n",
    "    print(f\"Age shape: {age_dummy.shape}\")\n",
    "    print(f\"Gender shape: {gender_dummy.shape}\")\n",
    "    print(f\"Ethnicity shape: {ethnicity_dummy.shape}\")\n",
    "    print(f\"Types shape: {types_dummy.shape}\")\n",
    "    print(f\"Codes shape: {codes_dummy.shape}\")\n",
    "\n",
    "    discharge = \"\"\n",
    "    discharge_token = text_tokenizer(discharge, return_tensors=\"pt\", padding=True, truncation=True, max_length=256) # 256 in the paper\n",
    "    note_flag = torch.tensor([1])\n",
    "\n",
    "    labvectors = torch.zeros(1, 114).unsqueeze(0)\n",
    "    lab_flag = torch.tensor([1])\n",
    "\n",
    "    label = torch.tensor([1])\n",
    "    print(f\"Discharge input_ids shape: {discharge_token['input_ids'].shape}\")\n",
    "    print(f\"Discharge attention_mask shape: {discharge_token['attention_mask'].shape}\")\n",
    "    print(f\"Labvectors shape: {labvectors.shape}\")\n",
    "    print(f\"Label shape: {label.shape}\")\n",
    "\n",
    "    input_dict = {\n",
    "        'age' : age_dummy,\n",
    "        'gender' : gender_dummy,\n",
    "        'ethnicity' : ethnicity_dummy,\n",
    "        'types' : types_dummy,\n",
    "        'codes' : codes_dummy,\n",
    "        'tabular_flag' : tabular_flag,\n",
    "        'discharge' : discharge_token,\n",
    "        'note_flag' : note_flag,\n",
    "        'labvectors' : labvectors,\n",
    "        'lab_flag' : lab_flag,\n",
    "        'label' : label    \n",
    "    }\n",
    "\n",
    "    with open(dummy_input_dict_path, 'wb') as f:\n",
    "        import pickle\n",
    "        pickle.dump(input_dict, f)  \n",
    "    print(f\"Dummy data saved to {dummy_input_dict_path}\")\n",
    "    \n",
    "with open(dummy_input_dict_path, 'rb') as f:\n",
    "    input_dict = pickle.load(f)  \n",
    "print(f\"Dummy data loaded from {dummy_input_dict_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "998c2f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy data loaded from Results/Dummy/emb_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "dummy_emb_dict_path = Path('Results/Dummy/emb_dict.pkl')\n",
    "dummy_emb_dict_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not dummy_emb_dict_path.exists() :\n",
    "    \n",
    "    emb_dict = {\n",
    "        'admission_id' : torch.zeros(1, 128),\n",
    "        'tab_feature' : torch.zeros(1, 128),\n",
    "        'tab_flag' : torch.zeros(1),\n",
    "        'lab_feature' : torch.zeros(1, 128),\n",
    "        'lab_flag' : torch.zeros(1),\n",
    "        'note_feature' : torch.zeros(1, 128),\n",
    "        'note_flag' : torch.zeros(1),\n",
    "        'label' : torch.zeros(1),   \n",
    "    }\n",
    "\n",
    "    with open(dummy_emb_dict_path, 'wb') as f:\n",
    "        import pickle\n",
    "        pickle.dump(emb_dict, f)  \n",
    "    print(f\"Dummy data saved to {dummy_emb_dict_path}\")\n",
    "    \n",
    "with open(dummy_emb_dict_path, 'rb') as f:\n",
    "    emb_dict = pickle.load(f)  \n",
    "print(f\"Dummy data loaded from {dummy_emb_dict_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8102eab7",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "681ef1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.tokenizer import MIMIC4Tokenizer\n",
    "from src.utils.config import load_cfg\n",
    "from argparse import Namespace\n",
    "tokenizer = MIMIC4Tokenizer()\n",
    "experiment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30648a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.core.e2e_mm_linear import Linear_Trainer\n",
    "# # Tabular\n",
    "# config_path = \"configs/E2E_MM_mortality_90days_BS_512_hidden_128.yaml\"\n",
    "# config = load_cfg(config_path, args=Namespace(fusion_method='WeightedFusion'))\n",
    "# model = Linear_Trainer(config, tokenizer, experiment).to('cuda')\n",
    "# model = model.linear_backbone\n",
    "\n",
    "\n",
    "# Sum WeightedFusion AttnMaskedFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a91e9500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ETF Matrix from Results/ETF/ETF_128_IN_2_OUT_Seed_2026.pt\n"
     ]
    }
   ],
   "source": [
    "from src.core.mm_linear import Linear_Trainer\n",
    "# Tabular\n",
    "config_path = \"configs/MM_mortality_90days_BS_512_hidden_128.yaml\"\n",
    "config = load_cfg(config_path, args=Namespace(fusion_method='WeightedFusion'))\n",
    "model = Linear_Trainer(config, experiment).to('cuda')\n",
    "model = model.linear_backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6786368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACs: 0.0 FLOPs: 0.0 Params: 0\n",
      "FLOPs (in MFLOPs): 0.00\n",
      "Params (in M): 0.00\n",
      "0.0 0.0 0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "# FLOPS\n",
    "model.eval()\n",
    "try : \n",
    "    macs, params = profile(model, inputs=input_dict, verbose=False)\n",
    "except : \n",
    "    macs, params = profile(model, inputs=emb_dict, verbose=False)\n",
    "flops = macs * 2\n",
    "print(\"MACs:\", macs, \"FLOPs:\", flops, \"Params:\", params)\n",
    "\n",
    "# conver to GB\n",
    "flops_mb = flops / 1e6\n",
    "params_mb = params / 1e6\n",
    "print(f\"FLOPs (in MFLOPs): {flops_mb:.2f}\")\n",
    "print(f\"Params (in M): {params_mb:.2f}\")\n",
    "\n",
    "\n",
    "print(macs, flops, params, round(flops_mb,2), round(params_mb,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2af4a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MACs</th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>Params</th>\n",
       "      <th>FLOPs (in MFLOPs)</th>\n",
       "      <th>Params (in M)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tabular</td>\n",
       "      <td>11368448</td>\n",
       "      <td>22736896</td>\n",
       "      <td>510464</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lab</td>\n",
       "      <td>210944</td>\n",
       "      <td>421888</td>\n",
       "      <td>210688</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>note</td>\n",
       "      <td>824320</td>\n",
       "      <td>1648640</td>\n",
       "      <td>429824</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E2E_SUM</td>\n",
       "      <td>12420096</td>\n",
       "      <td>24840192</td>\n",
       "      <td>1167488</td>\n",
       "      <td>24.84</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E2E_WeightedFusion</td>\n",
       "      <td>12403712</td>\n",
       "      <td>24807424</td>\n",
       "      <td>1150976</td>\n",
       "      <td>24.80</td>\n",
       "      <td>1.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E2E_Attn</td>\n",
       "      <td>12453256</td>\n",
       "      <td>24906512</td>\n",
       "      <td>1200643</td>\n",
       "      <td>24.90</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MUSE</td>\n",
       "      <td>83533056</td>\n",
       "      <td>167066112</td>\n",
       "      <td>1391105</td>\n",
       "      <td>167.07</td>\n",
       "      <td>1.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SUM</td>\n",
       "      <td>32768</td>\n",
       "      <td>65536</td>\n",
       "      <td>16512</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WeightedFusion</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AttnMaskedFusion</td>\n",
       "      <td>99088</td>\n",
       "      <td>198176</td>\n",
       "      <td>49667</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model      MACs      FLOPs   Params  FLOPs (in MFLOPs)  \\\n",
       "0             tabular  11368448   22736896   510464              20.00   \n",
       "1                 lab    210944     421888   210688               0.42   \n",
       "2                note    824320    1648640   429824               1.65   \n",
       "3             E2E_SUM  12420096   24840192  1167488              24.84   \n",
       "4  E2E_WeightedFusion  12403712   24807424  1150976              24.80   \n",
       "5            E2E_Attn  12453256   24906512  1200643              24.90   \n",
       "6                MUSE  83533056  167066112  1391105             167.07   \n",
       "7                 SUM     32768      65536    16512               0.07   \n",
       "8      WeightedFusion         0          0        0               0.00   \n",
       "9    AttnMaskedFusion     99088     198176    49667               0.20   \n",
       "\n",
       "   Params (in M)  \n",
       "0           0.51  \n",
       "1           0.21  \n",
       "2           0.43  \n",
       "3           1.17  \n",
       "4           1.15  \n",
       "5           1.20  \n",
       "6           1.39  \n",
       "7           0.02  \n",
       "8           0.00  \n",
       "9           0.05  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\"\"\"\n",
    "                        MACs\t        FLOPs\t        Params\t        FLOPs (in MFLOPs)\tParams (in M)\n",
    "tabular\t                11368448.0\t    22736896.0\t    510464.0\t    20.00\t            0.51\n",
    "lab\t                    210944.0\t    421888.0\t    210688.0\t    0.42\t            0.21\n",
    "note\t                824320.0\t    1648640.0\t    429824.0\t    1.65\t            0.43\n",
    "E2E_SUM\t                12420096.0\t    24840192.0\t    1167488.0\t    24.84\t            1.17\n",
    "E2E_WeightedFusion      12403712.0      24807424.0      1150976.0       24.80               1.15\n",
    "E2E_Attn                12453256.0      24906512.0      1200643.0       24.90               1.20\n",
    "SUM                     32768.0         65536.0         16512.0         0.07                0.02\n",
    "WeightedFusion          0               0               0.0             0.0                 0.0\n",
    "AttnMaskedFusion        99088.0         198176.0        49667.0         0.2                 0.05\n",
    "MUSE\t                83533056.0\t    167066112.0\t    1391105.0\t    167.07\t            1.39\n",
    "\"\"\"\n",
    "\n",
    "result_df = [\n",
    "    ['tabular', 11368448, 22736896, 510464, 20.00, 0.51],\n",
    "    ['lab', 210944, 421888, 210688, 0.42, 0.21],\n",
    "    ['note', 824320, 1648640, 429824, 1.65, 0.43],\n",
    "    ['E2E_SUM', 12420096, 24840192, 1167488, 24.84, 1.17],\n",
    "    ['E2E_WeightedFusion', 12403712, 24807424, 1150976, 24.80, 1.15],\n",
    "    ['E2E_Attn', 12453256, 24906512, 1200643, 24.90, 1.20],\n",
    "    ['MUSE', 83533056, 167066112, 1391105, 167.07, 1.39],\n",
    "    ['SUM', 32768, 65536, 16512, 0.07, 0.02],\n",
    "    ['WeightedFusion', 0, 0, 0, 0.0, 0.0],\n",
    "    ['AttnMaskedFusion', 99088, 198176, 49667, 0.2, 0.05],\n",
    "]\n",
    "\n",
    "result_df = pd.DataFrame(result_df, columns=['Model', 'MACs', 'FLOPs', 'Params', 'FLOPs (in MFLOPs)', 'Params (in M)'])\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43f848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIMIC Venv",
   "language": "python",
   "name": "mimic_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
