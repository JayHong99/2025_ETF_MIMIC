{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefbc9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, log_loss, brier_score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e78d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a55290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task='mortality'\n",
    "# days = '90days'\n",
    "\n",
    "task='readmission'\n",
    "days = '15days'\n",
    "\n",
    "modality_order = [\n",
    "    'Tabular', 'Lab', 'Note',\n",
    "    'E2E Fusion - Sum', 'E2E Fusion - Weighted Sum', 'E2E Fusion - Attn Masked',\n",
    "    'Simple Average', # Total\n",
    "    # 'Fusion - Sum', 'Fusion - Weighted Sum', 'Fusion - Attn Masked' # Fusion\n",
    "]\n",
    "\n",
    "\n",
    "df = pd.read_csv(Path('Results') / f\"Whole_{task.capitalize()}_{days}.csv\")\n",
    "                  \n",
    "ours_scores = df.set_index('Modality').loc[modality_order].reset_index()\n",
    "ours_scores = ours_scores[['Modality', 'ROC-AUC', 'PR-AUC', 'NLL Loss', 'Brier Loss']]\n",
    "ours_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e8508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keys(adm_ids_path) : \n",
    "    adm_ids = pickle.load(open(adm_ids_path, 'rb'))\n",
    "    code_ids, lab_ids, note_ids = adm_ids['test_code_ids'], adm_ids['test_lab_ids'], adm_ids['test_discharge_ids']\n",
    "    code_ids = np.array(code_ids).astype(int)\n",
    "    lab_ids = np.array(lab_ids).astype(int)\n",
    "    note_ids = np.array(note_ids).astype(int)\n",
    "    return set(code_ids), set(lab_ids), set(note_ids)\n",
    "\n",
    "def get_score(df ,ids) : \n",
    "    target_df = df.query('pid in @ids')\n",
    "    labels = target_df['label'].values\n",
    "    probs = target_df['score'].values\n",
    "    roc_auc = roc_auc_score(labels, probs)\n",
    "    precision, recall, _ = precision_recall_curve(labels, probs)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    nll_loss = log_loss(labels, probs)\n",
    "    brier_loss = brier_score_loss(labels, probs)\n",
    "    return roc_auc, pr_auc, len(labels), nll_loss, brier_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f64f9b7",
   "metadata": {},
   "source": [
    "# Get MUSE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc760d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "muse_dir = Path('previous_study/MUSE/Results/MUSE/output')\n",
    "muse_score_path = muse_dir / f'MUSE_{task}_{days}_scores.csv'\n",
    "data_dir = Path('/home/data/2025_MIMICIV_processed/mimic4/')\n",
    "\n",
    "if muse_score_path.exists() : \n",
    "    muse_scores = pd.read_csv(muse_score_path)\n",
    "\n",
    "else : \n",
    "    suffix = '-finetune'\n",
    "    output_dirs = list(muse_dir.glob(f'*{task}*{suffix}'))\n",
    "\n",
    "    total_output = []\n",
    "    for output_dir in output_dirs : \n",
    "        prediction_path = output_dir / 'predictions.txt'\n",
    "        prediction = pd.read_csv(prediction_path, header=None)\n",
    "        prediction.columns = ['pid', 'label', 'score']\n",
    "        \n",
    "        seed = int(prediction_path.parent.stem.split('-')[-2])\n",
    "        \n",
    "        adm_ids_path = data_dir / f\"task:{task}_{days}/admission_ids_seed_{seed}.pkl\"\n",
    "        adm_ids = pickle.load(open(adm_ids_path, 'rb'))\n",
    "        code_ids, lab_ids, note_ids = load_keys(adm_ids_path)\n",
    "        \n",
    "        # ## ['Tab All', 'Lab All', 'Note All', 'Tab Only', 'Tab + Lab', 'Tab + Note', 'Tab + Lab + Note']\n",
    "        tab_all_keys = np.array(list(code_ids))\n",
    "        total_output.append([seed, 'Tabular', *get_score(prediction, tab_all_keys)])\n",
    "        lab_all_keys = np.array(list(lab_ids))\n",
    "        total_output.append([seed, 'Lab', *get_score(prediction, lab_all_keys)])\n",
    "        note_all_keys = np.array(list(note_ids))\n",
    "        total_output.append([seed, 'Note', *get_score(prediction, note_all_keys)])\n",
    "        \n",
    "        tab_only_keys = np.array(list( # Tabular에만 있고 나머지는 차집합\n",
    "            code_ids - lab_ids - note_ids\n",
    "        ))\n",
    "        total_output.append([seed, 'Tabular Only', *get_score(prediction, tab_only_keys)])\n",
    "        \n",
    "        \n",
    "        tab_lab_keys = np.array(list( # Tab + Lab - Note\n",
    "            lab_ids - note_ids\n",
    "        ))\n",
    "        total_output.append([seed, 'Tab + Lab', *get_score(prediction, tab_lab_keys)])\n",
    "        \n",
    "        tab_note_keys = np.array(list( # Tab + Note - Lab\n",
    "            note_ids - lab_ids\n",
    "        ))\n",
    "        total_output.append([seed, 'Tab + Note', *get_score(prediction, tab_note_keys)])\n",
    "        \n",
    "        mm_keys = np.array(list(\n",
    "            code_ids & lab_ids & note_ids\n",
    "        ))\n",
    "        total_output.append([seed, 'Multimodal', *get_score(prediction, mm_keys)])\n",
    "        \n",
    "        all_keys = np.array(list(\n",
    "            code_ids | lab_ids | note_ids\n",
    "        ))\n",
    "        total_output.append([seed, 'Total', *get_score(prediction, all_keys)])\n",
    "\n",
    "    total_df = pd.DataFrame(total_output, columns = ['seed', 'Modality', 'AUROC', 'AUPRC', 'N', 'NLL', 'Brier'])\n",
    "    total_df_avg = total_df.groupby(['Modality']).mean().reset_index().drop(columns = ['seed'])\n",
    "    total_df_avg['N'] = total_df_avg['N'].astype(int)\n",
    "    total_df_avg[['AUROC', 'AUPRC']] = total_df_avg[['AUROC', 'AUPRC']].apply(lambda x : round(x*100, 2))\n",
    "    total_df_avg[['NLL', 'Brier']] = total_df_avg[['NLL', 'Brier']].apply(lambda x : round(x, 4))\n",
    "\n",
    "    last = total_df_avg.iloc[-1]\n",
    "    # remove last row and add modified last row\n",
    "    total_df_avg.loc[len(total_df_avg)-1] = ['Simple Average'] + list(last[1:] )\n",
    "    total_df_avg.loc[len(total_df_avg)] = ['Fusion - Sum'] + list(last[1:])\n",
    "    total_df_avg.loc[len(total_df_avg)] = ['Fusion - Weighted Sum'] + list(last[1:])\n",
    "    total_df_avg.loc[len(total_df_avg)] = ['Fusion - Attn Masked'] + list(last[1:])\n",
    "    total_df_avg\n",
    "\n",
    "    muse_scores = total_df_avg.set_index('Modality').loc[modality_order].reset_index()\n",
    "    muse_scores.to_csv(muse_score_path, index=False)\n",
    "muse_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a963d066",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659fe31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 'AUROC'\n",
    "merged = pd.merge(\n",
    "    ours_scores[['Modality', score]],\n",
    "    muse_scores[['Modality', score]],\n",
    "    on='Modality',\n",
    "    suffixes=('_ours', '_muse')\n",
    ")\n",
    "merged[f'Delta {score} (ours - muse)'] = merged[f'{score}_ours'] - merged[f'{score}_muse']\n",
    "merged = merged.set_index('Modality').loc[modality_order].reset_index()\n",
    "merged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robust Venv",
   "language": "python",
   "name": "robust_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
